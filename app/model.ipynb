{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9c55b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fastapi uvicorn python-multipart\n",
    "!pip install langchain langchain-community\n",
    "!pip install sentence-transformers\n",
    "!pip install faiss-cpu\n",
    "!pip install PyMuPDF\n",
    "!pip install python-dotenv\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07c46e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [17832]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Advanced RAG PDF Query System...\n",
      "Using Groq API with model: llama3-8b-8192\n",
      "Server will be available at: http://localhost:8000\n",
      "Make sure to set your GROQ_API_KEY in the .env file\n",
      "Starting server in background...\n",
      "Server started successfully!\n",
      "Backend API endpoints available at: http://localhost:8000/api/\n",
      "Open your frontend at: http://localhost:8000/static/\n",
      "To stop the server, restart the kernel or interrupt the notebook\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:53313 - \"GET / HTTP/1.1\" 404 Not Found\n",
      "INFO:     127.0.0.1:53314 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
      "INFO:     127.0.0.1:53319 - \"GET /api/status HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53337 - \"GET /api/health HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53339 - \"GET /api/status HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "# Advanced RAG Backend with Groq API\n",
    "# This notebook contains the RAG backend functionality without frontend\n",
    "\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "import uvicorn\n",
    "from fastapi import FastAPI, File, UploadFile, Form, HTTPException\n",
    "from fastapi.responses import JSONResponse\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "import fitz  # PyMuPDF\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "import openai\n",
    "import nest_asyncio\n",
    "import threading\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        load_dotenv()\n",
    "        self.GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "        if not self.GROQ_API_KEY:\n",
    "            raise ValueError(\"GROQ_API_KEY not found in environment variables\")\n",
    "        \n",
    "        self.VECTOR_STORE_PATH = \"data/index\"\n",
    "        self.UPLOAD_PATH = \"data/uploads\"\n",
    "        self.PROCESSED_PATH = \"data/processed\"\n",
    "        self.MODEL_NAME = \"llama3-8b-8192\"  # Groq model\n",
    "        self.EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        self.CHUNK_SIZE = 500\n",
    "        self.CHUNK_OVERLAP = 50\n",
    "        self.MAX_RETRIEVED_CHUNKS = 5\n",
    "        \n",
    "        # Create directories\n",
    "        os.makedirs(self.UPLOAD_PATH, exist_ok=True)\n",
    "        os.makedirs(self.PROCESSED_PATH, exist_ok=True)\n",
    "        os.makedirs(self.VECTOR_STORE_PATH, exist_ok=True)\n",
    "\n",
    "config = Config()\n",
    "\n",
    "class PDFParser:\n",
    "    @staticmethod\n",
    "    def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "        \"\"\"Extract text from PDF using PyMuPDF\"\"\"\n",
    "        try:\n",
    "            doc = fitz.open(pdf_path)\n",
    "            text = \"\"\n",
    "            \n",
    "            for page_num in range(doc.page_count):\n",
    "                page = doc[page_num]\n",
    "                page_text = page.get_text()\n",
    "                \n",
    "                # Basic preprocessing\n",
    "                page_text = page_text.replace('\\n\\n', '\\n')\n",
    "                page_text = page_text.strip()\n",
    "                \n",
    "                if page_text:\n",
    "                    text += page_text + \"\\n\\n\"\n",
    "            \n",
    "            doc.close()\n",
    "            return text.strip()\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error extracting text from PDF: {str(e)}\")\n",
    "\n",
    "class TextChunker:\n",
    "    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "    \n",
    "    def chunk_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into chunks\"\"\"\n",
    "        chunks = self.text_splitter.split_text(text)\n",
    "        return [chunk.strip() for chunk in chunks if chunk.strip()]\n",
    "\n",
    "class Embedder:\n",
    "    def __init__(self, model_name: str):\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=model_name,\n",
    "            model_kwargs={'device': 'cpu'}\n",
    "        )\n",
    "        self.vector_store = None\n",
    "    \n",
    "    def create_vector_store(self, chunks: List[str], pdf_filename: str) -> FAISS:\n",
    "        \"\"\"Create FAISS vector store from text chunks\"\"\"\n",
    "        documents = [\n",
    "            Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\"source\": pdf_filename, \"chunk_id\": i}\n",
    "            )\n",
    "            for i, chunk in enumerate(chunks)\n",
    "        ]\n",
    "        \n",
    "        self.vector_store = FAISS.from_documents(documents, self.embeddings)\n",
    "        return self.vector_store\n",
    "    \n",
    "    def save_vector_store(self, path: str):\n",
    "        \"\"\"Save vector store to disk\"\"\"\n",
    "        if self.vector_store:\n",
    "            self.vector_store.save_local(path)\n",
    "    \n",
    "    def load_vector_store(self, path: str) -> Optional[FAISS]:\n",
    "        \"\"\"Load vector store from disk\"\"\"\n",
    "        try:\n",
    "            self.vector_store = FAISS.load_local(\n",
    "                path, \n",
    "                self.embeddings,\n",
    "                allow_dangerous_deserialization=True\n",
    "            )\n",
    "            return self.vector_store\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "class Retriever:\n",
    "    def __init__(self, vector_store: FAISS, embeddings):\n",
    "        self.vector_store = vector_store\n",
    "        self.embeddings = embeddings\n",
    "    \n",
    "    def retrieve_similar_chunks(self, query: str, k: int = 5) -> List[Document]:\n",
    "        \"\"\"Retrieve k most similar chunks for the query\"\"\"\n",
    "        try:\n",
    "            docs = self.vector_store.similarity_search(query, k=k)\n",
    "            return docs\n",
    "        except Exception as e:\n",
    "            print(f\"Error in retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "class LLMInterface:\n",
    "    def __init__(self, api_key: str, model_name: str):\n",
    "        self.client = openai.OpenAI(\n",
    "            base_url=\"https://api.groq.com/openai/v1\",\n",
    "            api_key=api_key\n",
    "        )\n",
    "        self.model_name = model_name\n",
    "    \n",
    "    def generate_answer(self, context: str, question: str) -> str:\n",
    "        \"\"\"Generate answer using Groq API with hallucination prevention\"\"\"\n",
    "        \n",
    "        # Enhanced prompt to prevent hallucination\n",
    "        prompt = f\"\"\"You are a helpful assistant that answers questions based STRICTLY on the provided context. \n",
    "\n",
    "IMPORTANT INSTRUCTIONS:\n",
    "1. Only use information from the provided context to answer the question\n",
    "2. If the answer is not in the context, say \"I cannot find this information in the provided document\"\n",
    "3. Do not add information from your general knowledge\n",
    "4. Be specific and cite relevant parts of the context when possible\n",
    "5. If the context is unclear or insufficient, acknowledge this limitation\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer based solely on the context provided:\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a precise document assistant that only answers based on provided context.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=512,\n",
    "                temperature=0.1,  # Low temperature to reduce hallucination\n",
    "                top_p=0.9,\n",
    "                frequency_penalty=0.0,\n",
    "                presence_penalty=0.0\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        \n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "\n",
    "class RAGApplication:\n",
    "    def __init__(self):\n",
    "        self.config = config\n",
    "        self.pdf_parser = PDFParser()\n",
    "        self.chunker = TextChunker(\n",
    "            chunk_size=self.config.CHUNK_SIZE,\n",
    "            chunk_overlap=self.config.CHUNK_OVERLAP\n",
    "        )\n",
    "        self.embedder = Embedder(self.config.EMBEDDING_MODEL)\n",
    "        self.llm = LLMInterface(self.config.GROQ_API_KEY, self.config.MODEL_NAME)\n",
    "        self.current_retriever = None\n",
    "        self.current_pdf_name = None\n",
    "    \n",
    "    async def process_pdf(self, pdf_file: UploadFile) -> dict:\n",
    "        try:\n",
    "            # Save uploaded file\n",
    "            pdf_path = os.path.join(self.config.UPLOAD_PATH, pdf_file.filename)\n",
    "            with open(pdf_path, \"wb\") as f:\n",
    "                content = await pdf_file.read()\n",
    "                f.write(content)\n",
    "            \n",
    "            # Extract text\n",
    "            text = self.pdf_parser.extract_text_from_pdf(pdf_path)\n",
    "            \n",
    "            if not text.strip():\n",
    "                raise Exception(\"No text found in PDF\")\n",
    "            \n",
    "            # Chunk text\n",
    "            chunks = self.chunker.chunk_text(text)\n",
    "            \n",
    "            if not chunks:\n",
    "                raise Exception(\"No valid chunks created from PDF\")\n",
    "            \n",
    "            # Create vector store\n",
    "            vector_store = self.embedder.create_vector_store(chunks, pdf_file.filename)\n",
    "            \n",
    "            # Save vector store\n",
    "            vector_store_path = os.path.join(self.config.VECTOR_STORE_PATH, \"faiss_index\")\n",
    "            self.embedder.save_vector_store(vector_store_path)\n",
    "            \n",
    "            # Create retriever\n",
    "            self.current_retriever = Retriever(vector_store, self.embedder.embeddings)\n",
    "            self.current_pdf_name = pdf_file.filename\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"message\": f\"PDF '{pdf_file.filename}' processed successfully\",\n",
    "                \"chunks_created\": len(chunks),\n",
    "                \"text_length\": len(text)\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"message\": f\"Error processing PDF: {str(e)}\"\n",
    "            }\n",
    "    \n",
    "    def query_document(self, question: str) -> dict:\n",
    "        \"\"\"Query the processed document\"\"\"\n",
    "        try:\n",
    "            if not self.current_retriever:\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"message\": \"No document has been processed yet. Please upload a PDF first.\"\n",
    "                }\n",
    "            \n",
    "            # Retrieve similar chunks\n",
    "            retrieved_docs = self.current_retriever.retrieve_similar_chunks(\n",
    "                question, \n",
    "                k=self.config.MAX_RETRIEVED_CHUNKS\n",
    "            )\n",
    "            \n",
    "            if not retrieved_docs:\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"message\": \"No relevant information found in the document.\"\n",
    "                }\n",
    "            \n",
    "            # Prepare context\n",
    "            context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "            \n",
    "            # Generate answer\n",
    "            answer = self.llm.generate_answer(context, question)\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"answer\": answer,\n",
    "                \"sources\": len(retrieved_docs),\n",
    "                \"document\": self.current_pdf_name\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"message\": f\"Error querying document: {str(e)}\"\n",
    "            }\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI(title=\"Advanced RAG PDF Query System\", version=\"1.0.0\")\n",
    "\n",
    "# Add CORS middleware\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Mount static files for frontend\n",
    "app.mount(\"/static\", StaticFiles(directory=\"../frontend\"), name=\"static\")\n",
    "\n",
    "# Initialize RAG application\n",
    "rag_app = RAGApplication()\n",
    "\n",
    "@app.post(\"/api/upload\")\n",
    "async def upload_pdf(file: UploadFile = File(...)):\n",
    "    \"\"\"Upload and process PDF file\"\"\"\n",
    "    if not file.filename.lower().endswith('.pdf'):\n",
    "        raise HTTPException(status_code=400, detail=\"Only PDF files are allowed\")\n",
    "    \n",
    "    result = await rag_app.process_pdf(file)\n",
    "    return JSONResponse(content=result)\n",
    "\n",
    "@app.post(\"/api/query\")\n",
    "async def query_document(question: str = Form(...)):\n",
    "    \"\"\"Query the processed document\"\"\"\n",
    "    result = rag_app.query_document(question)\n",
    "    return JSONResponse(content=result)\n",
    "\n",
    "@app.get(\"/api/health\")\n",
    "async def health_check():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    return {\"status\": \"healthy\", \"message\": \"RAG system is running\"}\n",
    "\n",
    "@app.get(\"/api/status\")\n",
    "async def get_status():\n",
    "    \"\"\"Get current system status\"\"\"\n",
    "    return {\n",
    "        \"document_loaded\": rag_app.current_pdf_name is not None,\n",
    "        \"current_document\": rag_app.current_pdf_name,\n",
    "        \"model\": config.MODEL_NAME,\n",
    "        \"embedding_model\": config.EMBEDDING_MODEL\n",
    "    }\n",
    "\n",
    "# Enable nested event loops for Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "def run_server():\n",
    "    \"\"\"Run the FastAPI server in a separate thread\"\"\"\n",
    "    uvicorn.run(\n",
    "        app,\n",
    "        host=\"0.0.0.0\",\n",
    "        port=8000,\n",
    "        reload=False,\n",
    "        log_level=\"info\"\n",
    "    )\n",
    "\n",
    "# Start the server\n",
    "print(\"Starting Advanced RAG PDF Query System...\")\n",
    "print(\"Using Groq API with model: llama3-8b-8192\")\n",
    "print(\"Server will be available at: http://localhost:8000\")\n",
    "print(\"Make sure to set your GROQ_API_KEY in the .env file\")\n",
    "print(\"Starting server in background...\")\n",
    "\n",
    "# Run server in a separate thread\n",
    "server_thread = threading.Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "# Give the server time to start\n",
    "time.sleep(3)\n",
    "print(\"Server started successfully!\")\n",
    "print(\"Backend API endpoints available at: http://localhost:8000/api/\")\n",
    "print(\"Open your frontend at: http://localhost:8000/static/\")\n",
    "print(\"To stop the server, restart the kernel or interrupt the notebook\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
